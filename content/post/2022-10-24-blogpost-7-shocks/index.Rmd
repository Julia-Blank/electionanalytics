---
title: 'Blogpost 7: Shocks'
author: Julia Blank
date: '2022-10-24'
slug: []
categories: []
tags: []
---

```{r,echo = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE) 
# Loading in necessary libraries

library(tidyverse)
library(ggplot2)
library(stargazer)
library(janitor)
library(readxl)

library(tidyverse)
library(ggplot2)
library(blogdown)
library(stargazer)
library(readr)
library(usmap)
library(rmapshaper)
library(sf)
library(janitor)
library(tigris)
library(leaflet)

```

##

Hi everyone! We’re back this week with and just two weeks out from election day! This week we covered a lot of topics but especially shocks! Shocks are bound to happen in any election for many reasons, whether it be the volatility of the economy, foreign policy events, or personal scandals brought to light by the media. The very nature of elections is unpredictable and shocks are bound to happen, hence the infamous "October Surprise". I posited quite a few ways to include shocks in my model, but ultimately the expectation that shocks are regular in every election cycle and the inherently incalculable nature of the effect of specific shocks made me rule out including any additional calculation or variable for shock in my model. I believed that any significant way by which they should be calculated would be baked into expert models that are in my expert prediction variable so any additional fitting to shock would be counter intuitive and possibly overfit the model to shock. 

As such, moving towards a final prediction this week the big question I aimed to investigate was: What additional variables remain unexplored that could allow my model to better forecast the 2022 midterm elections?


WHY I DIDN'T USE A POOLED MODEL? 
To start off, I want to address this weeks extension. I debated putting together a pooled model for this week but I realized a few reasons why it may not be worth it. First off, a pooled model works best when borrowing data from districts that have more data have the possibility of supporting districts with less data. However, the variation of the amount of data for each district was never more than 3 data points. Since every district had so few data points, borrowing data doesn't get me much farther than I was before. Additionally, the problem with my district level model over all is that the data is just so scarce that the model is not really generalizable. As such, I chose to forgo creating a pooled model in favor of exploring other predictors. 


```{r, warning = FALSE, message = FALSE}
expert_ratings <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022-10-03-blog-post-4-expert-prediction-and-incumbency/04-Incumbency-9-29-/Section-data/District-level-forecast/expert_rating.csv")

historical_results <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022-10-03-blog-post-4-expert-prediction-and-incumbency/04-Incumbency-9-29-/Section-data/District-level-forecast/house party vote share by district 1948-2020.csv") %>%
  clean_names()

incumb_dist_1948_2020_3_ <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022_10_17_blog_post_6_the_ground_game/06_Ground-Game_10-13/Section data/incumb_dist_1948-2020 (3).csv")

roper_cong_polls_1979_2022 <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022-10-03-blog-post-4-expert-prediction-and-incumbency/04-Incumbency-9-29-/roper_cong_polls_1979-2022.csv")

cvap_district<- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022_10_17_blog_post_6_the_ground_game/06_Ground-Game_10-13/Section data/cvap_district_2012-2020_clean.csv")

polls_df <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022_10_17_blog_post_6_the_ground_game/06_Ground-Game_10-13/Section data/house_polls_long.csv")

party_power <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022-10-24-blogpost-7-shocks/party_power.csv")

pres_approval_gallup_1941_2022 <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/03-Polling (9-22)/section data/Other polling data/pres_approval_gallup_1941-2022.csv")

seats <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022-10-24-blogpost-7-shocks/Seats and Elections - Sheet1.csv")

committeefunding <- read_csv("/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022-10-24-blogpost-7-shocks/Funding - Sheet1.csv")

```

```{r}

##CLEAN DATA SETS TO PUT THEM TOGETHER 

avg_ratings <- expert_ratings %>% 
  select(year, state, district, avg_rating)
  
party_power <- party_power %>%
  rename(year = Year)

dem_results_new <- incumb_dist_1948_2020_3_ %>%
    mutate(dem_status = case_when(DemStatus == "Incumbent" ~ 1,
                               TRUE ~ 0),
         rep_status = case_when(RepStatus == "Incumbent" ~ 1,
                               TRUE ~ 0))

cvap_district <- cvap_district %>%
  rename(st_cd_fips = geoid) 

polls_cvap_df <- merge(polls_df, cvap_district, by = c('st_cd_fips', 'year'))

polls_cvap_vp_df <- merge(polls_cvap_df, dem_results_new, by = c('st_cd_fips', 'year')) %>%
    rename(state_name = state.x) %>%
    mutate(totalvotes = RepVotes + DemVotes,
         turnout = (totalvotes/cvap)*100) %>%
    mutate(DemVotesMajorPct = DemVotesMajorPercent/100,
         RepVotesMajorPct = RepVotesMajorPercent/100) %>%
  #FILTER OUT UNCONTESTED SEATS
  filter(!is.na(DemCandidate), !is.na(RepCandidate)) %>%
  mutate(DemVotesMajorPct = DemVotesMajorPercent/100,
         RepVotesMajorPct = RepVotesMajorPercent/100)

s <- which(avg_ratings$district < 10)

avg_ratings_clean <- avg_ratings %>%
  mutate(cd_fips = case_when(as.numeric(district) < 10 ~ paste0("0", district),
                                TRUE ~ district)) %>%
  rename(state_name = state) %>%
  mutate(district = case_when(
    district == "AL" ~ "1",
    TRUE ~ district
  ))  %>%
  drop_na()
  

final_data <- polls_cvap_vp_df %>%
  left_join(avg_ratings_clean, by = c("state_name", "cd_fips", "year")) %>%
    left_join(party_power, by = "year")
final_data$DEM <- as.numeric(final_data$DEM)
final_data$REP <- as.numeric(final_data$REP)

train_data_dem <- final_data %>%
  filter(year != 2022) %>% 
  group_by(st_cd_fips) %>%
  filter(n() > 1) %>% # Filtering out single data rows
  drop_na() %>%
  group_nest() %>% 
  mutate(data = map(data, ~unnest(., cols = c())))
  
train_data_rep <- final_data %>%
  filter(year != 2022) %>% 
  group_by(st_cd_fips) %>%
  filter(n() > 1) %>% # Filtering out single data rows
  group_nest() %>% 
  mutate(data = map(data, ~unnest(., cols = c()))) 
```

```{r, warning = FALSE, message = FALSE}
cvap_vp_df <- merge(dem_results_new, cvap_district, by = c('st_cd_fips', 'year'))

cvap_vp_rate_df <- cvap_vp_df %>%
  mutate(totalvotes = RepVotes + DemVotes,
         turnout = (totalvotes/cvap)*100) %>%
  rename(state_name = state.x, 
         district = cd) %>%
  left_join(avg_ratings_clean, by = c("state_name", "district", "year")) %>%
  filter(year != 2022) %>%
  drop_na() %>%
  left_join(party_power, by = "year")

cvap_midterm <- cvap_vp_rate_df %>%
  filter(year %in% c(2010, 2014, 2018))
```

To being my discussion of this weeks new models, I wanted to illustrate the best models from last week. After testing measures of turnout in national and district-level models, I determined my best national models predicted two party vote share by using measures of turnout, average expert prediction, and party presidential incumbency. In addition, I ran this model on all election years in the data set as well as on just the midterm election years. The regression results in the table below illustrate the success of the model. As can be seen, the model performed better on the midterm election data set than on the whole data set but that is likely due to smaller sample size of the midterm election dataset. 

```{r}
## CURRENT LEADING MODELS
bestnational_all<- lm(DemVotesMajorPercent ~ turnout + avg_rating + dem_status, cvap_vp_rate_df)


bestnational_midterm <- lm(DemVotesMajorPercent ~ turnout + avg_rating + dem_status, cvap_midterm)

stargazer(bestnational_all,bestnational_midterm, type = "text",
          column.labels = c("All election years model", "Midterm years model"),
          covariate.labels = c("Turnout", "Expert Rating", "Incumbent Dem President"),
          header=FALSE,
          single.row = TRUE,
          no.space = TRUE,
          column.sep.width = "1pt",
          font.size = "small")
          
```


As I feel pretty good about these linear models in forecasting, I want to use the model as a base for my final model and take this week to investigate whether a few other variables serve as strong predictors within this already successful framework. After doing extensive research, it appears that number of contested seats for a party in the election has some correlation with vote percentage. Traditional academic wisdom describes an inverse relationship between number of seats a party must defend and their vote percentage. To investigate this on my own, however, I plotted the regression for both the democratic and republican party on number  of seats to defend and party vote share. The plot is pictured below. 


```{r}

seats <- seats %>%
  rename(year = Year)


seatsfinal <- cvap_vp_rate_df %>%
  left_join(seats, by = "year") %>%
  rename(d_seats_up = `D Party Seats Up`,
         r_seats_up = `R Party Seats Up`)

##SEATS PLOT
seats_regression <- seatsfinal %>%
  ggplot() +
  geom_point(aes(x = d_seats_up, y = DemVotesMajorPercent, color = "Democrats")) +
  geom_smooth(aes(x = d_seats_up, y = DemVotesMajorPercent, color = "Democrats"), method = "lm") + 
  geom_point(aes(x = r_seats_up, y = RepVotesMajorPercent, color = "Republicans")) + 
  geom_smooth(aes(x = r_seats_up, y = RepVotesMajorPercent, color = "Republicans"), method = "lm") +
  xlab("Party Seats Up") +
  ylab("Party Vote Share") +
  labs(title = "Linear Regression of Party Vote Share vs. Party Seats Up for Election") +
  scale_color_manual(name = "Regression",             # legend name
                     values = c("Democrats" = "blue",  # map regression line colors
                                "Republicans" = "red"))

  seats_regression
```
As illustrated above, while small, there is a small negative correlation between party vote share and number of seats up for contest. My concern is that this correlation is small so that include seat share in my model will not have much of an effect. To test this theory further, I added number of seats a party has up for election into my model. Thus my model is meant to predict Democratic Vote Share through using average of expert predictions, turnout, whether a sitting president is a Democrat, and the number of seats the democrat hold in the house right now. The results of the model are illustrated in the table below.  

```{r, warning = FALSE, message = FALSE}
seatssimple <- lm(DemVotesMajorPercent ~ turnout + avg_rating + dem_status + d_seats_up , seatsfinal)

seatsinteractlm <- lm(DemVotesMajorPercent ~ turnout + avg_rating + dem_status + d_seats_up + dem_status*d_seats_up, seatsfinal)

seatsnoturnoutlm <- lm(DemVotesMajorPercent ~ avg_rating + dem_status + d_seats_up + dem_status*d_seats_up, seatsfinal)


#stargazer(seatssimple, seatsinteractlm, type = "text",
          #title="Seats Linear Models Regression Results", dep.var.labels = "Democratic Vote Percentage",
          #column.labels	= c("Seats Inclusive Model", "Seat Interaction Model"),
          #covariate.labels = c("Turnout", "Expert Rating", "Incumbent Dem President", 'Dem Seats', "Interaction of sitting party president and seats"),
         # header=FALSE,
         # single.row = TRUE,
          #no.space = TRUE,
          #column.sep.width = "1pt",
          #font.size = "small")


#seatsrep <- lm(RepVotesMajorPercent ~ turnout + avg_rating + rep_status + r_seats_up , seatsfinal)

#stargazer(seatsrep, type = "text")
```


![](/Users/juliablank/Documents/GOV 50 PROJECTS/electionanalytics/content/post/2022-10-24-blogpost-7-shocks/bp7 ss1.png)


As can be seen in the models above adding in seats does improve both the r-squared and adjusted r-squared measures of the model. In the first model, I just added the number of democratic seats as an addition variable to my base model. This yielded an adjusted r-squared value of 0.737 which is a marginal improvement on the adjusted r-squared value of 0.720 from my previous national model tested on both midterm and presidential election years. Interestingly, in the model, the incumbent president variable decreased in significance. Additionally, I thought especially for midterm elections that whether the sitting president is from a specific party might have influence on the effect of seats to defend in the house. As such, I created a second model that is the same as the first but now includes an interaction term between number of democratic seats to defend and the incumbent president variable. This interaction term is somewhat significant with a p-value less than 0.05. The resulting model has the highest adjusted r-squared of an national level model tested on all election years that I've made. However, this model is only mildly 0.03 more in adjusted r-squared than the one with just seats alone. Moving forward, I'm going to look at the tradeoffs of generalization versus accuracy in these models to decide on which to use.    

After this examination, I wanted to look at one more variable that could be a potentially valuable predictor in my model: committee campaign finance. I wanted to look into campaign finance by party close to the election as money is a physical committment to supporting a party or individual. I believed that seeing the amount of energy measured via financing for a party would help me predict party vote share better closer to an election. While originally I wanted to look at the amount of money each party committee raised in the 30 days closest to the election, the only data I could find was total money raised by each party's national committee. Using this variable, I made a model predicting Democratic Vote Share with turnout, average expert rating, democratic president incumbency status, number of seats the democratic part holds, and DCCC total funding for congressional races. The summary of the model is depicted in the table below. 

```{r, warning = FALSE, message = FALSE}
committeefunding<- committeefunding %>%
  rename(year = Year)

funding <- seatsfinal %>%
  left_join(committeefunding, by = "year")

fundedlm <- lm(DemVotesMajorPercent ~ turnout + avg_rating + dem_status + d_seats_up + D_funding, funding)

stargazer(fundedlm, type = "text",
          title="Seats Linear Models Regression Results", dep.var.labels = "Democratic Vote Percentage",
          covariate.labels = c("Turnout", "Expert Rating", "Incumbent Dem President", 'Dem Seats', 'DCCC Funding'),
          header=FALSE,
          single.row = TRUE,
          no.space = TRUE,
          column.sep.width = "1pt",
          font.size = "small")
```

As you can see in the above model, the adjusted r-squared decreased in comparison to previous models. Once again, financing proves to be a week predictor of party vote share and is not an effective predictor in my models. I am glad, however, that I was able to rule out the predictor one and for all. 

```{r}
##test national midterm model 
avg_rate_test <- avg_ratings_clean %>%
  filter(year == 2022) %>%
  mutate(mean(avg_rating))

midtermturnout <- cvap_midterm %>%
  mutate(mean(turnout))

test_data_nat <- data.frame(turnout = 47.57791, 
         avg_support_dem = 45.3,
         mavg_support_rep = 44.9, 
         dem_status = 1, 
         rep_status = 0, 
         d_seats_up = 220,
         avg_rating = 4.070922)

dem_vote_share <- predict(seatssimple, test_data_nat)


```

Using my simple model that included democrat seat number in the house(without the interaction term), my prediction is that the Democrats will win approximately 47.6% of the vote. 


In conclusion, for my final model seats held by a given party does prove to be an important predictor. This improves my model and I will take it in the future. Similarly, after evaluating accessible metrics of financing, I can't find any that prove to be effective predictors of party vote percentage or electoral success. In future analyses, I'm curious if having the right metric would make the variable more predictive or  if monetary contributions in general is not really what makes a successful campaign. Perhaps campaigns only need to meet a certain monetary threshold and anything after is helpful but not necessary for changing electoral outcomes. All in all, I'm glad I took this week to do some additional inquiry with various variables to discover and rule out more of them as effective predictors of elections. 

Citations:
“The 2022 Midterm Elections: What the Historical Data Suggest.: The American Presidency Project.” The 2022 Midterm Elections: What the Historical Data Suggest. | The American Presidency Project, 30 Aug. 2022, https://www.presidency.ucsb.edu/analyses/the-2022-midterm-elections-what-the-historical-data-suggest. 

“House Party Committees' Sources of Funds, 1999-2018.” Campaign Finance Data, http://www.cfinst.org/data.aspx. 

Jacobson, Gary C. “ The 2022 U.S. Midterm Election: A Conventional Referendum or Something Different ?” Dropbox, University of California, San Diego, https://www.dropbox.com/s/95s7hip8bzg5x8n/Jacobson%202022%20Essay.pdf?dl=0. 

